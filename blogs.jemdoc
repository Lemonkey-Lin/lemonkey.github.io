# jemdoc: menu{MENU}{Blogs.html}, nofooter  
== Blogs
This page is about some research that I'm interested in, as well as my thoughts on it. The topics are diverse, and some of them may be incorrect or naive.

== Roadmap of multiple hypothesis testing

Testing millions of hypothesis simultaneously is commonly seen in various areas. To effectively report true discoveries, Yoav Benjamini and Yosef Hochberg  proposed a new criteria *FDR* in 1995 along with [http://www.math.tau.ac.il/~ybenja/MyPapers/benjamini_hochberg1995.pdf BH procedure] that controls FDR for independent and PRDS p-values. Multiple hypothesis testing has been a prominent issue ever since. 


- Later on, [https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2004.00439.x  John D. Storey, Jonathan E. Taylor, David Siegmund
] proposed a *martingale view* of BH procedure and also proposed the *pi_0 estimation* problem. The martingale theory turns out to be a wonderful tool for constructing valid procedure under independency. Various procedures based on this theory includes [https://arxiv.org/abs/1606.07926 SABHA], [https://arxiv.org/abs/1606.01969 Seq++], [https://arxiv.org/abs/1609.06035 AdaPT]


- dependency problem: Scientists discovered the independent or PRDS assumption of *BH procedure* is too restricted and rarely appear in practice. 
[https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-4/The-control-of-the-false-discovery-rate-in-multiple-testing/10.1214/aos/1013699998.full Yoav Benjamini, Daniel Yekutieli] proposed *BY procedure*, which guarantees FDR control for arbitrary dependent p-values, yet with a big sacrifice of power. Later on, [https://arxiv.org/abs/0802.1406  Gilles Blanchard , Etienne Roquain ] generalizes this to *step-up procedure*. But still, these procedures are so conservative that greatly hinder their popularity for a long time. Until recently, [https://arxiv.org/abs/2007.10438 Fithian and Lei] made a big breakthrough by using *conditional calibration technique*. Other procedure like *e-BH* by [https://arxiv.org/pdf/2009.02824.pdf Ruodu Wang, Aaditya Ramdas] also works for dependent statistics. But contrary to Ruodu's conjecture, I think e-BH procedure can even hardly beat BY procedure (See my discussion below)

- Covariate assisted procedure: 

- Bayesian framwork: There's a very beautiful view of BH procedure --- the bayesian two group model. 
Efron said /"It is always a good sign when a statistical procedure enjoys both a frequentist and Bayesian support, and the BH algorithm passes the test."/

[https://www.jstor.org/stable/20441304  False discovery control with p-value weighting]



[https://arxiv.org/abs/1701.05179  Covariate-powered weighted multiple testing with false discovery rate control.]



[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3175141/  False discovery rate control with groups.]



== Causal Inference
One thing that makes me unease when I learn causal inference is its *non-testable assumptions*, which turns out to be the indispensable element for causal inference. 

Indeed, Rubin created a valid language to help us answer causal questions with rigorous math, which I personally believe is also the only way to tackle causal problems. I mean, can you do inference outside the framework Rubin built? Honestly saying, I can't imagine reliable inference without the concept of *potential outcome*. 

However, I found this framework to be troublesome when evaluating the performance of models. Because establishing *criteria of correctness* is hard, let alone calculating it, guaranteeing it. Take the easy propensity score model as an example, people assumed this model will fit an machine learning algorithm to calculate propensity scores and then plug in the estimator of average treatment effect. But, how can you guarantee your model has good property? *Does your algorithm guarantee some type I error?* I believe statistics is not black box, not automatic. But rather, one shall always keep mathematical guarantee in mind instead of doing things intuitively. 

To make a breakthrough in these problems seems difficult, because the model itself is based on some *non testable assumptions*. Nevertheless, I appreciate the greatness and beauty Rubin built in causal inference. At least one thing I learned is that causal is really uneasy.

== Conformal Inference


== Knockoff


== A Power Analysis for knockoff

Power analysis in multiple hypothesis is hard, analysis for sophisticated statistics like LASSO is even more difficult. 
In fact, inference based on LASSO statistics is not well addressed until [https://arxiv.org/abs/1404.5609  Barber and Candes(2015) ] made a breakthrough by proposing knockoff procedure. 

But how well does it work? Power analysis for knockoff seems to rely on solely empirical results. 

A powerful tool I learned in [https://www.stat.berkeley.edu/~songmei/Teaching/STAT260_Spring2021/schedule.html STAT 260 (Mean Field Asymptotics)]  provides an oracle picture of FDR-TPP trade-off under some specific settings (asymptotic, pi_0 known). With this tool in hand, we are able to compare the FDR and TPP in knockoff procedure and the "oracle procedure" to understand the performance of knockoff. 


This is exactly what these papers are about.
- [https://arxiv.org/abs/1712.06465 A Power and Prediction Analysis for Knockoffs with Lasso Statistics] 
- [https://arxiv.org/abs/2007.15346 A Power Analysis for Knockoffs with the Lasso Coefficient-Difference Statistic
]

== A note on e-values


== Bayesian statistics
I'm obsessed with viewing things in geometry prospective. One of the most beautiful theorem I learned in bayesian statistics is 

/Every admissible estimator is a (possibly randomized) Bayes estimator for some prior.

where admissible estimator means/


== Robust statistics, resilience
Check out [Mengqi_Lin_Challenge_Problem_Sets.pdf my solutions to the challgenging problems] for fun!

